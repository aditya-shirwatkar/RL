{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from matplotlib import animation as anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = []\n",
    "yval = []\n",
    "yavg = []\n",
    "def animate(i):\n",
    "    plt.cla()\n",
    "    plt.plot(xval, yval, 'r*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 420)\n",
    "        self.fc2 = nn.Linear(420, 128)\n",
    "        self.fc3 = nn.Linear(128, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda:0\n"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnAcrobot:\n",
    "    def __init__(self, d_size, episodes):\n",
    "        self.D_SIZE = d_size\n",
    "        self.EPISODES = episodes\n",
    "        self.model = Agent().to(device)\n",
    "        self.optim = optim.Adam(self.model.parameters(),  lr=8e-3)\n",
    "        self.trajectory = []\n",
    "        self.batch_states = []\n",
    "        self.batch_actions = []\n",
    "        self.batch_returns = []\n",
    "        self.batch_Rtau = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.probs = []\n",
    "\n",
    "    def makePolicy(self, s):\n",
    "        logits = self.model(s.to(device))\n",
    "        # print(logits)\n",
    "        return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    def takeAction(self, s):\n",
    "        # s = self.model(torch.as_tensor(s).float().unsqueeze(0))\n",
    "        # prob = torch.distributions.Categorical(logits=s)\n",
    "        # act = prob.sample()\n",
    "        # # act = torch.argmax(s)\n",
    "        # # print(prob.log_prob(act))\n",
    "        # # self.log_probs[indx].append(prob.log_prob(act))\n",
    "        # return act.item(), prob.log_prob(act), s[0][act]\n",
    "        return self.makePolicy(s).sample().item()\n",
    "\n",
    "    def computeGradientJ(self, s, act, rTau):\n",
    "        # print(s)\n",
    "        logp = self.makePolicy(s).log_prob(act)\n",
    "        # print(rTau, logp)\n",
    "        return -(logp * rTau).mean()\n",
    "\n",
    "    def train(self):\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.computeGradientJ(torch.as_tensor(self.batch_states).float().to(device),\n",
    "                                     torch.as_tensor(self.batch_actions).int().to(device),\n",
    "                                     torch.as_tensor(self.batch_Rtau).float().to(device))\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        g = self.batch_Rtau[0]\n",
    "        self.batch_Rtau = []\n",
    "        self.batch_actions = []\n",
    "        self.batch_states = []\n",
    "        self.batch_returns = []\n",
    "        return g, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "acro = LearnAcrobot(420, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s at 283 is  tensor(39.2807, device='cuda:0', grad_fn=<NegBackward>)  And return is 31.0\nLoss at 284 is  tensor(43.2466, device='cuda:0', grad_fn=<NegBackward>)  And return is 67.0\nLoss at 285 is  tensor(41.7434, device='cuda:0', grad_fn=<NegBackward>)  And return is 66.0\nLoss at 286 is  tensor(45.8020, device='cuda:0', grad_fn=<NegBackward>)  And return is 36.0\nLoss at 287 is  tensor(30.9479, device='cuda:0', grad_fn=<NegBackward>)  And return is 45.0\nLoss at 288 is  tensor(37.1864, device='cuda:0', grad_fn=<NegBackward>)  And return is 57.0\nLoss at 289 is  tensor(40.5546, device='cuda:0', grad_fn=<NegBackward>)  And return is 78.0\nLoss at 290 is  tensor(46.6521, device='cuda:0', grad_fn=<NegBackward>)  And return is 63.0\nLoss at 291 is  tensor(57.1484, device='cuda:0', grad_fn=<NegBackward>)  And return is 133.0\nLoss at 292 is  tensor(29.3186, device='cuda:0', grad_fn=<NegBackward>)  And return is 70.0\nLoss at 293 is  tensor(43.5671, device='cuda:0', grad_fn=<NegBackward>)  And return is 45.0\nLoss at 294 is  tensor(42.8707, device='cuda:0', grad_fn=<NegBackward>)  And return is 60.0\nLoss at 295 is  tensor(45.1650, device='cuda:0', grad_fn=<NegBackward>)  And return is 40.0\nLoss at 296 is  tensor(61.3465, device='cuda:0', grad_fn=<NegBackward>)  And return is 27.0\nLoss at 297 is  tensor(88.3248, device='cuda:0', grad_fn=<NegBackward>)  And return is 85.0\nLoss at 298 is  tensor(33.3429, device='cuda:0', grad_fn=<NegBackward>)  And return is 78.0\nLoss at 299 is  tensor(46.5226, device='cuda:0', grad_fn=<NegBackward>)  And return is 91.0\nLoss at 300 is  tensor(43.7035, device='cuda:0', grad_fn=<NegBackward>)  And return is 14.0\nLoss at 301 is  tensor(46.8775, device='cuda:0', grad_fn=<NegBackward>)  And return is 122.0\nLoss at 302 is  tensor(39.1632, device='cuda:0', grad_fn=<NegBackward>)  And return is 99.0\nLoss at 303 is  tensor(34.0347, device='cuda:0', grad_fn=<NegBackward>)  And return is 48.0\nLoss at 304 is  tensor(34.1325, device='cuda:0', grad_fn=<NegBackward>)  And return is 78.0\nLoss at 305 is  tensor(96.5917, device='cuda:0', grad_fn=<NegBackward>)  And return is 19.0\nLoss at 306 is  tensor(34.6970, device='cuda:0', grad_fn=<NegBackward>)  And return is 41.0\nLoss at 307 is  tensor(58.6378, device='cuda:0', grad_fn=<NegBackward>)  And return is 21.0\nLoss at 308 is  tensor(42.6677, device='cuda:0', grad_fn=<NegBackward>)  And return is 112.0\nLoss at 309 is  tensor(41.3401, device='cuda:0', grad_fn=<NegBackward>)  And return is 34.0\nLoss at 310 is  tensor(64.9525, device='cuda:0', grad_fn=<NegBackward>)  And return is 132.0\nLoss at 311 is  tensor(51.1989, device='cuda:0', grad_fn=<NegBackward>)  And return is 55.0\nLoss at 312 is  tensor(69.3062, device='cuda:0', grad_fn=<NegBackward>)  And return is 26.0\nLoss at 313 is  tensor(45.8328, device='cuda:0', grad_fn=<NegBackward>)  And return is 45.0\nLoss at 314 is  tensor(44.1233, device='cuda:0', grad_fn=<NegBackward>)  And return is 78.0\nLoss at 315 is  tensor(76.1762, device='cuda:0', grad_fn=<NegBackward>)  And return is 122.0\nLoss at 316 is  tensor(94.9319, device='cuda:0', grad_fn=<NegBackward>)  And return is 170.0\nLoss at 317 is  tensor(76.4605, device='cuda:0', grad_fn=<NegBackward>)  And return is 171.0\nLoss at 318 is  tensor(47.6138, device='cuda:0', grad_fn=<NegBackward>)  And return is 15.0\nLoss at 319 is  tensor(46.0415, device='cuda:0', grad_fn=<NegBackward>)  And return is 43.0\nLoss at 320 is  tensor(62.7424, device='cuda:0', grad_fn=<NegBackward>)  And return is 64.0\nLoss at 321 is  tensor(99.2789, device='cuda:0', grad_fn=<NegBackward>)  And return is 210.0\nLoss at 322 is  tensor(62.3925, device='cuda:0', grad_fn=<NegBackward>)  And return is 90.0\nLoss at 323 is  tensor(46.1836, device='cuda:0', grad_fn=<NegBackward>)  And return is 78.0\nLoss at 324 is  tensor(73.6825, device='cuda:0', grad_fn=<NegBackward>)  And return is 81.0\nLoss at 325 is  tensor(60.1255, device='cuda:0', grad_fn=<NegBackward>)  And return is 115.0\nLoss at 326 is  tensor(61.0513, device='cuda:0', grad_fn=<NegBackward>)  And return is 30.0\nLoss at 327 is  tensor(84.0505, device='cuda:0', grad_fn=<NegBackward>)  And return is 38.0\nLoss at 328 is  tensor(66.6306, device='cuda:0', grad_fn=<NegBackward>)  And return is 97.0\nLoss at 329 is  tensor(53.2731, device='cuda:0', grad_fn=<NegBackward>)  And return is 86.0\nLoss at 330 is  tensor(68.0887, device='cuda:0', grad_fn=<NegBackward>)  And return is 123.0\nLoss at 331 is  tensor(48.2687, device='cuda:0', grad_fn=<NegBackward>)  And return is 22.0\nLoss at 332 is  tensor(54.7961, device='cuda:0', grad_fn=<NegBackward>)  And return is 71.0\nLoss at 333 is  tensor(75.4450, device='cuda:0', grad_fn=<NegBackward>)  And return is 40.0\nLoss at 334 is  tensor(57.9045, device='cuda:0', grad_fn=<NegBackward>)  And return is 49.0\nLoss at 335 is  tensor(55.4534, device='cuda:0', grad_fn=<NegBackward>)  And return is 54.0\nLoss at 336 is  tensor(65.0097, device='cuda:0', grad_fn=<NegBackward>)  And return is 110.0\nLoss at 337 is  tensor(58.1939, device='cuda:0', grad_fn=<NegBackward>)  And return is 82.0\nLoss at 338 is  tensor(56.0586, device='cuda:0', grad_fn=<NegBackward>)  And return is 72.0\nLoss at 339 is  tensor(37.5249, device='cuda:0', grad_fn=<NegBackward>)  And return is 44.0\nLoss at 340 is  tensor(89.8143, device='cuda:0', grad_fn=<NegBackward>)  And return is 26.0\nLoss at 341 is  tensor(32.8190, device='cuda:0', grad_fn=<NegBackward>)  And return is 31.0\nLoss at 342 is  tensor(31.9350, device='cuda:0', grad_fn=<NegBackward>)  And return is 74.0\nLoss at 343 is  tensor(37.9916, device='cuda:0', grad_fn=<NegBackward>)  And return is 28.0\nLoss at 344 is  tensor(44.0679, device='cuda:0', grad_fn=<NegBackward>)  And return is 74.0\nLoss at 345 is  tensor(34.7923, device='cuda:0', grad_fn=<NegBackward>)  And return is 39.0\nLoss at 346 is  tensor(60.6477, device='cuda:0', grad_fn=<NegBackward>)  And return is 176.0\nLoss at 347 is  tensor(38.4970, device='cuda:0', grad_fn=<NegBackward>)  And return is 80.0\nLoss at 348 is  tensor(26.5346, device='cuda:0', grad_fn=<NegBackward>)  And return is 17.0\nLoss at 349 is  tensor(33.1204, device='cuda:0', grad_fn=<NegBackward>)  And return is 93.0\nLoss at 350 is  tensor(28.1895, device='cuda:0', grad_fn=<NegBackward>)  And return is 67.0\nLoss at 351 is  tensor(34.6112, device='cuda:0', grad_fn=<NegBackward>)  And return is 34.0\nLoss at 352 is  tensor(33.0507, device='cuda:0', grad_fn=<NegBackward>)  And return is 60.0\nLoss at 353 is  tensor(36.1135, device='cuda:0', grad_fn=<NegBackward>)  And return is 32.0\nLoss at 354 is  tensor(32.8059, device='cuda:0', grad_fn=<NegBackward>)  And return is 49.0\nLoss at 355 is  tensor(38.3466, device='cuda:0', grad_fn=<NegBackward>)  And return is 48.0\nLoss at 356 is  tensor(29.7476, device='cuda:0', grad_fn=<NegBackward>)  And return is 55.0\nLoss at 357 is  tensor(31.2499, device='cuda:0', grad_fn=<NegBackward>)  And return is 32.0\nLoss at 358 is  tensor(43.5820, device='cuda:0', grad_fn=<NegBackward>)  And return is 72.0\nLoss at 359 is  tensor(46.8933, device='cuda:0', grad_fn=<NegBackward>)  And return is 29.0\nLoss at 360 is  tensor(46.5200, device='cuda:0', grad_fn=<NegBackward>)  And return is 24.0\nLoss at 361 is  tensor(38.7542, device='cuda:0', grad_fn=<NegBackward>)  And return is 58.0\nLoss at 362 is  tensor(38.5066, device='cuda:0', grad_fn=<NegBackward>)  And return is 29.0\nLoss at 363 is  tensor(44.2155, device='cuda:0', grad_fn=<NegBackward>)  And return is 74.0\nLoss at 364 is  tensor(41.7172, device='cuda:0', grad_fn=<NegBackward>)  And return is 26.0\nLoss at 365 is  tensor(37.1673, device='cuda:0', grad_fn=<NegBackward>)  And return is 51.0\nLoss at 366 is  tensor(41.7958, device='cuda:0', grad_fn=<NegBackward>)  And return is 33.0\nLoss at 367 is  tensor(52.3843, device='cuda:0', grad_fn=<NegBackward>)  And return is 54.0\nLoss at 368 is  tensor(53.2639, device='cuda:0', grad_fn=<NegBackward>)  And return is 29.0\nLoss at 369 is  tensor(76.7774, device='cuda:0', grad_fn=<NegBackward>)  And return is 131.0\nLoss at 370 is  tensor(73.3404, device='cuda:0', grad_fn=<NegBackward>)  And return is 171.0\nLoss at 371 is  tensor(55.2318, device='cuda:0', grad_fn=<NegBackward>)  And return is 49.0\nLoss at 372 is  tensor(58.4635, device='cuda:0', grad_fn=<NegBackward>)  And return is 26.0\nLoss at 373 is  tensor(35.9314, device='cuda:0', grad_fn=<NegBackward>)  And return is 90.0\nLoss at 374 is  tensor(32.9919, device='cuda:0', grad_fn=<NegBackward>)  And return is 55.0\nLoss at 375 is  tensor(38.7217, device='cuda:0', grad_fn=<NegBackward>)  And return is 46.0\nLoss at 376 is  tensor(40.2021, device='cuda:0', grad_fn=<NegBackward>)  And return is 85.0\nLoss at 377 is  tensor(53.2692, device='cuda:0', grad_fn=<NegBackward>)  And return is 47.0\nLoss at 378 is  tensor(48.1891, device='cuda:0', grad_fn=<NegBackward>)  And return is 35.0\nLoss at 379 is  tensor(42.9783, device='cuda:0', grad_fn=<NegBackward>)  And return is 39.0\nLoss at 380 is  tensor(36.5934, device='cuda:0', grad_fn=<NegBackward>)  And return is 16.0\nLoss at 381 is  tensor(64.5358, device='cuda:0', grad_fn=<NegBackward>)  And return is 19.0\nLoss at 382 is  tensor(39.9495, device='cuda:0', grad_fn=<NegBackward>)  And return is 114.0\nLoss at 383 is  tensor(60.5809, device='cuda:0', grad_fn=<NegBackward>)  And return is 80.0\nLoss at 384 is  tensor(49.8617, device='cuda:0', grad_fn=<NegBackward>)  And return is 44.0\nLoss at 385 is  tensor(72.8766, device='cuda:0', grad_fn=<NegBackward>)  And return is 56.0\nLoss at 386 is  tensor(43.0545, device='cuda:0', grad_fn=<NegBackward>)  And return is 65.0\nLoss at 387 is  tensor(46.0833, device='cuda:0', grad_fn=<NegBackward>)  And return is 70.0\nLoss at 388 is  tensor(51.4407, device='cuda:0', grad_fn=<NegBackward>)  And return is 125.0\nLoss at 389 is  tensor(43.9659, device='cuda:0', grad_fn=<NegBackward>)  And return is 77.0\nLoss at 390 is  tensor(52.6325, device='cuda:0', grad_fn=<NegBackward>)  And return is 69.0\nLoss at 391 is  tensor(51.2643, device='cuda:0', grad_fn=<NegBackward>)  And return is 38.0\nLoss at 392 is  tensor(38.6200, device='cuda:0', grad_fn=<NegBackward>)  And return is 103.0\nLoss at 393 is  tensor(48.1723, device='cuda:0', grad_fn=<NegBackward>)  And return is 125.0\nLoss at 394 is  tensor(52.1741, device='cuda:0', grad_fn=<NegBackward>)  And return is 139.0\nLoss at 395 is  tensor(55.0897, device='cuda:0', grad_fn=<NegBackward>)  And return is 79.0\nLoss at 396 is  tensor(58.9669, device='cuda:0', grad_fn=<NegBackward>)  And return is 85.0\nLoss at 397 is  tensor(79.4808, device='cuda:0', grad_fn=<NegBackward>)  And return is 167.0\nLoss at 398 is  tensor(50.7592, device='cuda:0', grad_fn=<NegBackward>)  And return is 74.0\nLoss at 399 is  tensor(47.9381, device='cuda:0', grad_fn=<NegBackward>)  And return is 102.0\nLoss at 400 is  tensor(48.8833, device='cuda:0', grad_fn=<NegBackward>)  And return is 75.0\nLoss at 401 is  tensor(49.1894, device='cuda:0', grad_fn=<NegBackward>)  And return is 24.0\nLoss at 402 is  tensor(59.5443, device='cuda:0', grad_fn=<NegBackward>)  And return is 45.0\nLoss at 403 is  tensor(67.1767, device='cuda:0', grad_fn=<NegBackward>)  And return is 79.0\nLoss at 404 is  tensor(63.4894, device='cuda:0', grad_fn=<NegBackward>)  And return is 51.0\nLoss at 405 is  tensor(64.2316, device='cuda:0', grad_fn=<NegBackward>)  And return is 57.0\nLoss at 406 is  tensor(58.9481, device='cuda:0', grad_fn=<NegBackward>)  And return is 93.0\nLoss at 407 is  tensor(59.9613, device='cuda:0', grad_fn=<NegBackward>)  And return is 126.0\nLoss at 408 is  tensor(81.4929, device='cuda:0', grad_fn=<NegBackward>)  And return is 57.0\nLoss at 409 is  tensor(33.5524, device='cuda:0', grad_fn=<NegBackward>)  And return is 62.0\nLoss at 410 is  tensor(46.5717, device='cuda:0', grad_fn=<NegBackward>)  And return is 50.0\nLoss at 411 is  tensor(44.6925, device='cuda:0', grad_fn=<NegBackward>)  And return is 85.0\nLoss at 412 is  tensor(35.1085, device='cuda:0', grad_fn=<NegBackward>)  And return is 38.0\nLoss at 413 is  tensor(37.1092, device='cuda:0', grad_fn=<NegBackward>)  And return is 52.0\nLoss at 414 is  tensor(106.3397, device='cuda:0', grad_fn=<NegBackward>)  And return is 88.0\nLoss at 415 is  tensor(56.2132, device='cuda:0', grad_fn=<NegBackward>)  And return is 46.0\nLoss at 416 is  tensor(36.3264, device='cuda:0', grad_fn=<NegBackward>)  And return is 73.0\nLoss at 417 is  tensor(45.1617, device='cuda:0', grad_fn=<NegBackward>)  And return is 51.0\nLoss at 418 is  tensor(43.1645, device='cuda:0', grad_fn=<NegBackward>)  And return is 53.0\nLoss at 419 is  tensor(41.3378, device='cuda:0', grad_fn=<NegBackward>)  And return is 39.0\nLoss at 420 is  tensor(37.5546, device='cuda:0', grad_fn=<NegBackward>)  And return is 126.0\nLoss at 421 is  tensor(39.7785, device='cuda:0', grad_fn=<NegBackward>)  And return is 35.0\nLoss at 422 is  tensor(31.2028, device='cuda:0', grad_fn=<NegBackward>)  And return is 25.0\nLoss at 423 is  tensor(35.8615, device='cuda:0', grad_fn=<NegBackward>)  And return is 75.0\nLoss at 424 is  tensor(56.1255, device='cuda:0', grad_fn=<NegBackward>)  And return is 94.0\nLoss at 425 is  tensor(38.0184, device='cuda:0', grad_fn=<NegBackward>)  And return is 61.0\nLoss at 426 is  tensor(31.4451, device='cuda:0', grad_fn=<NegBackward>)  And return is 89.0\nLoss at 427 is  tensor(44.1477, device='cuda:0', grad_fn=<NegBackward>)  And return is 40.0\nLoss at 428 is  tensor(39.4823, device='cuda:0', grad_fn=<NegBackward>)  And return is 55.0\nLoss at 429 is  tensor(31.4702, device='cuda:0', grad_fn=<NegBackward>)  And return is 48.0\nLoss at 430 is  tensor(37.1515, device='cuda:0', grad_fn=<NegBackward>)  And return is 20.0\nLoss at 431 is  tensor(36.7968, device='cuda:0', grad_fn=<NegBackward>)  And return is 22.0\nLoss at 432 is  tensor(26.0925, device='cuda:0', grad_fn=<NegBackward>)  And return is 15.0\nLoss at 433 is  tensor(41.7727, device='cuda:0', grad_fn=<NegBackward>)  And return is 45.0\nLoss at 434 is  tensor(61.9643, device='cuda:0', grad_fn=<NegBackward>)  And return is 54.0\nLoss at 435 is  tensor(46.2086, device='cuda:0', grad_fn=<NegBackward>)  And return is 39.0\nLoss at 436 is  tensor(27.9997, device='cuda:0', grad_fn=<NegBackward>)  And return is 36.0\nLoss at 437 is  tensor(36.6553, device='cuda:0', grad_fn=<NegBackward>)  And return is 42.0\nLoss at 438 is  tensor(75.6555, device='cuda:0', grad_fn=<NegBackward>)  And return is 47.0\nLoss at 439 is  tensor(37.8433, device='cuda:0', grad_fn=<NegBackward>)  And return is 50.0\nLoss at 440 is  tensor(31.1254, device='cuda:0', grad_fn=<NegBackward>)  And return is 35.0\nLoss at 441 is  tensor(55.7331, device='cuda:0', grad_fn=<NegBackward>)  And return is 71.0\nLoss at 442 is  tensor(53.7186, device='cuda:0', grad_fn=<NegBackward>)  And return is 48.0\nLoss at 443 is  tensor(53.2707, device='cuda:0', grad_fn=<NegBackward>)  And return is 44.0\nLoss at 444 is  tensor(82.6274, device='cuda:0', grad_fn=<NegBackward>)  And return is 56.0\nLoss at 445 is  tensor(57.0815, device='cuda:0', grad_fn=<NegBackward>)  And return is 142.0\nLoss at 446 is  tensor(44.2191, device='cuda:0', grad_fn=<NegBackward>)  And return is 20.0\nLoss at 447 is  tensor(56.5591, device='cuda:0', grad_fn=<NegBackward>)  And return is 38.0\nLoss at 448 is  tensor(68.3744, device='cuda:0', grad_fn=<NegBackward>)  And return is 168.0\nLoss at 449 is  tensor(47.3492, device='cuda:0', grad_fn=<NegBackward>)  And return is 26.0\nLoss at 450 is  tensor(42.4579, device='cuda:0', grad_fn=<NegBackward>)  And return is 110.0\nLoss at 451 is  tensor(59.7780, device='cuda:0', grad_fn=<NegBackward>)  And return is 113.0\nLoss at 452 is  tensor(41.0146, device='cuda:0', grad_fn=<NegBackward>)  And return is 73.0\nLoss at 453 is  tensor(50.2441, device='cuda:0', grad_fn=<NegBackward>)  And return is 36.0\nLoss at 454 is  tensor(43.1839, device='cuda:0', grad_fn=<NegBackward>)  And return is 15.0\nLoss at 455 is  tensor(35.8207, device='cuda:0', grad_fn=<NegBackward>)  And return is 67.0\nLoss at 456 is  tensor(41.1960, device='cuda:0', grad_fn=<NegBackward>)  And return is 25.0\nLoss at 457 is  tensor(48.6965, device='cuda:0', grad_fn=<NegBackward>)  And return is 103.0\nLoss at 458 is  tensor(57.6202, device='cuda:0', grad_fn=<NegBackward>)  And return is 127.0\nLoss at 459 is  tensor(48.0410, device='cuda:0', grad_fn=<NegBackward>)  And return is 87.0\nLoss at 460 is  tensor(65.9910, device='cuda:0', grad_fn=<NegBackward>)  And return is 124.0\nLoss at 461 is  tensor(68.0178, device='cuda:0', grad_fn=<NegBackward>)  And return is 41.0\nLoss at 462 is  tensor(50.4316, device='cuda:0', grad_fn=<NegBackward>)  And return is 85.0\nLoss at 463 is  tensor(55.2260, device='cuda:0', grad_fn=<NegBackward>)  And return is 100.0\nLoss at 464 is  tensor(49.0986, device='cuda:0', grad_fn=<NegBackward>)  And return is 23.0\nLoss at 465 is  tensor(68.7222, device='cuda:0', grad_fn=<NegBackward>)  And return is 77.0\nLoss at 466 is  tensor(53.6703, device='cuda:0', grad_fn=<NegBackward>)  And return is 85.0\nLoss at 467 is  tensor(43.4268, device='cuda:0', grad_fn=<NegBackward>)  And return is 53.0\nLoss at 468 is  tensor(51.0113, device='cuda:0', grad_fn=<NegBackward>)  And return is 122.0\nLoss at 469 is  tensor(53.1286, device='cuda:0', grad_fn=<NegBackward>)  And return is 164.0\nLoss at 470 is  tensor(40.2151, device='cuda:0', grad_fn=<NegBackward>)  And return is 53.0\nLoss at 471 is  tensor(43.9194, device='cuda:0', grad_fn=<NegBackward>)  And return is 55.0\nLoss at 472 is  tensor(40.2821, device='cuda:0', grad_fn=<NegBackward>)  And return is 32.0\nLoss at 473 is  tensor(48.0163, device='cuda:0', grad_fn=<NegBackward>)  And return is 124.0\nLoss at 474 is  tensor(45.2238, device='cuda:0', grad_fn=<NegBackward>)  And return is 17.0\nLoss at 475 is  tensor(50.1760, device='cuda:0', grad_fn=<NegBackward>)  And return is 125.0\nLoss at 476 is  tensor(42.5427, device='cuda:0', grad_fn=<NegBackward>)  And return is 76.0\nLoss at 477 is  tensor(26.7074, device='cuda:0', grad_fn=<NegBackward>)  And return is 47.0\nLoss at 478 is  tensor(40.3173, device='cuda:0', grad_fn=<NegBackward>)  And return is 28.0\nLoss at 479 is  tensor(36.8145, device='cuda:0', grad_fn=<NegBackward>)  And return is 49.0\nLoss at 480 is  tensor(32.8932, device='cuda:0', grad_fn=<NegBackward>)  And return is 78.0\nLoss at 481 is  tensor(38.0159, device='cuda:0', grad_fn=<NegBackward>)  And return is 67.0\nLoss at 482 is  tensor(83.2746, device='cuda:0', grad_fn=<NegBackward>)  And return is 36.0\nLoss at 483 is  tensor(36.8787, device='cuda:0', grad_fn=<NegBackward>)  And return is 59.0\nLoss at 484 is  tensor(60.4929, device='cuda:0', grad_fn=<NegBackward>)  And return is 153.0\nLoss at 485 is  tensor(38.2764, device='cuda:0', grad_fn=<NegBackward>)  And return is 44.0\nLoss at 486 is  tensor(33.3543, device='cuda:0', grad_fn=<NegBackward>)  And return is 61.0\nLoss at 487 is  tensor(63.7385, device='cuda:0', grad_fn=<NegBackward>)  And return is 146.0\nLoss at 488 is  tensor(46.4596, device='cuda:0', grad_fn=<NegBackward>)  And return is 122.0\nLoss at 489 is  tensor(35.8864, device='cuda:0', grad_fn=<NegBackward>)  And return is 21.0\nLoss at 490 is  tensor(30.7504, device='cuda:0', grad_fn=<NegBackward>)  And return is 35.0\nLoss at 491 is  tensor(29.5053, device='cuda:0', grad_fn=<NegBackward>)  And return is 19.0\nLoss at 492 is  tensor(39.3980, device='cuda:0', grad_fn=<NegBackward>)  And return is 13.0\nLoss at 493 is  tensor(40.3670, device='cuda:0', grad_fn=<NegBackward>)  And return is 66.0\nLoss at 494 is  tensor(29.6293, device='cuda:0', grad_fn=<NegBackward>)  And return is 66.0\nLoss at 495 is  tensor(60.8829, device='cuda:0', grad_fn=<NegBackward>)  And return is 49.0\nLoss at 496 is  tensor(55.6370, device='cuda:0', grad_fn=<NegBackward>)  And return is 59.0\nLoss at 497 is  tensor(55.6576, device='cuda:0', grad_fn=<NegBackward>)  And return is 21.0\nLoss at 498 is  tensor(30.7859, device='cuda:0', grad_fn=<NegBackward>)  And return is 24.0\nLoss at 499 is  tensor(33.4588, device='cuda:0', grad_fn=<NegBackward>)  And return is 78.0\n"
    }
   ],
   "source": [
    "win_count = 0\n",
    "render = True\n",
    "for i in range(1, acro.EPISODES):\n",
    "    \n",
    "    while len(acro.batch_states) < (acro.D_SIZE):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        eps_rew = []\n",
    "\n",
    "        while not done:\n",
    "            acro.batch_states.append(state.copy())\n",
    "\n",
    "            action = acro.takeAction(torch.as_tensor(state).float().unsqueeze(0))\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            acro.batch_actions.append(action)\n",
    "            eps_rew.append(reward)\n",
    "\n",
    "            if done:\n",
    "                acro.batch_returns.append(sum(eps_rew))\n",
    "                acro.batch_Rtau += [sum(eps_rew)] * len(eps_rew)\n",
    "            state = new_state\n",
    "            # if state[0] >= env.goal_position:\n",
    "            #     win_count += 1\n",
    "            #     print('we made it')\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "        render = False\n",
    "    # if i%100 == 0:\n",
    "    #     print(loss)\n",
    "    g, loss = acro.train()\n",
    "    # print(g)\n",
    "    xval.append(i)\n",
    "    yval.append(g)\n",
    "    yavg.append(sum(yval)/i)\n",
    "    # if i%20 ==0:\n",
    "    print(f'Loss at {i} is ', loss, f' And return is {g}')\n",
    "    render = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 375.2875 248.518125 \nL 375.2875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \nL 368.0875 7.2 \nL 33.2875 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m384f6eaf3e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.89451\" xlink:href=\"#m384f6eaf3e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(44.71326 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.011706\" xlink:href=\"#m384f6eaf3e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(99.467956 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.128902\" xlink:href=\"#m384f6eaf3e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(160.585152 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"231.246098\" xlink:href=\"#m384f6eaf3e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(221.702348 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"292.363294\" xlink:href=\"#m384f6eaf3e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(282.819544 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"353.48049\" xlink:href=\"#m384f6eaf3e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(343.93674 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mca134b4bf3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"219.837925\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 223.637143)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"194.430119\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 198.229338)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"169.022314\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 172.821532)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"143.614508\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 147.413727)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"118.206703\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 122.005921)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"92.798897\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 250 -->\n      <g transform=\"translate(7.2 96.598116)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"67.391091\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 300 -->\n      <g transform=\"translate(7.2 71.19031)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"41.983286\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 350 -->\n      <g transform=\"translate(7.2 45.782505)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mca134b4bf3\" y=\"16.57548\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 400 -->\n      <g transform=\"translate(7.2 20.374699)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pf0b24398f9)\" d=\"M 48.505682 207.642178 \nL 49.728026 205.609554 \nL 50.339198 201.544305 \nL 50.95037 214.756364 \nL 51.561542 204.593241 \nL 52.172714 210.691115 \nL 52.783886 193.921963 \nL 53.395058 190.36487 \nL 54.006229 199.003524 \nL 54.617401 188.840402 \nL 55.228573 188.332246 \nL 55.839745 190.36487 \nL 56.450917 201.544305 \nL 57.062089 197.479056 \nL 57.673261 207.642178 \nL 58.284433 186.299621 \nL 58.895605 179.185436 \nL 59.506777 202.052461 \nL 60.117949 183.758841 \nL 60.729121 180.201748 \nL 61.340293 193.413807 \nL 61.951465 168.514158 \nL 62.562637 203.576929 \nL 63.173809 179.185436 \nL 63.784981 173.595719 \nL 64.396153 180.709904 \nL 65.007325 208.150334 \nL 65.618497 163.432596 \nL 66.229669 200.527993 \nL 66.840841 149.204225 \nL 67.452013 190.36487 \nL 68.063185 200.019836 \nL 68.674357 157.842879 \nL 69.285528 210.691115 \nL 69.8967 214.248208 \nL 70.507872 199.51168 \nL 71.119044 171.563094 \nL 71.730216 133.959542 \nL 72.341388 204.593241 \nL 72.95256 158.859191 \nL 73.563732 140.565571 \nL 74.174904 181.21806 \nL 74.786076 17.083636 \nL 75.397248 173.595719 \nL 76.00842 177.660968 \nL 76.619592 123.288264 \nL 77.230764 127.353513 \nL 77.841936 175.628343 \nL 78.453108 179.185436 \nL 79.06428 203.068773 \nL 79.675452 197.987212 \nL 80.286624 164.957065 \nL 80.897796 210.182959 \nL 81.508968 203.068773 \nL 82.12014 209.166646 \nL 82.731312 176.644655 \nL 83.342484 187.82409 \nL 83.953656 194.938275 \nL 84.564827 194.430119 \nL 85.175999 205.609554 \nL 86.398343 201.544305 \nL 87.009515 200.527993 \nL 87.620687 168.514158 \nL 88.231859 208.150334 \nL 88.843031 171.563094 \nL 89.454203 211.707427 \nL 90.065375 160.38366 \nL 90.676547 192.397495 \nL 91.287719 206.11771 \nL 91.898891 140.057415 \nL 92.510063 209.166646 \nL 93.121235 192.905651 \nL 93.732407 179.693592 \nL 94.343579 136.500323 \nL 94.954751 44.524066 \nL 95.565923 161.399972 \nL 96.177095 188.332246 \nL 96.788267 92.290741 \nL 97.399439 173.087563 \nL 98.010611 179.185436 \nL 98.621783 207.642178 \nL 99.232955 211.199271 \nL 99.844127 210.691115 \nL 100.455298 117.19039 \nL 101.06647 153.269474 \nL 101.677642 122.271951 \nL 102.288814 144.122664 \nL 102.899986 151.23685 \nL 103.511158 156.318411 \nL 104.12233 110.584361 \nL 104.733502 200.019836 \nL 105.344674 210.182959 \nL 105.955846 172.07125 \nL 106.567018 205.101398 \nL 107.17819 159.875504 \nL 107.789362 212.723739 \nL 108.400534 213.740051 \nL 109.011706 208.65849 \nL 109.622878 207.134022 \nL 110.845222 203.068773 \nL 111.456394 205.609554 \nL 112.067566 205.609554 \nL 112.678738 161.908128 \nL 113.28991 169.53047 \nL 113.901082 199.51168 \nL 114.512254 208.65849 \nL 115.123426 211.707427 \nL 115.734597 208.65849 \nL 116.345769 212.723739 \nL 116.956941 166.481533 \nL 117.568113 207.642178 \nL 118.179285 176.644655 \nL 118.790457 161.908128 \nL 119.401629 203.068773 \nL 120.012801 213.740051 \nL 120.623973 191.889339 \nL 121.235145 211.199271 \nL 121.846317 148.187913 \nL 122.457489 130.402449 \nL 123.679833 159.367348 \nL 124.291005 156.826567 \nL 124.902177 208.65849 \nL 125.513349 137.516635 \nL 126.124521 188.332246 \nL 126.735693 131.418761 \nL 127.346865 204.085085 \nL 127.958037 155.810255 \nL 128.569209 201.036149 \nL 129.180381 96.864146 \nL 129.791553 118.206703 \nL 130.402725 124.304576 \nL 131.013896 209.674803 \nL 131.625068 205.609554 \nL 132.23624 130.402449 \nL 132.847412 207.642178 \nL 133.458584 147.679757 \nL 134.069756 179.693592 \nL 134.680928 148.187913 \nL 135.2921 186.807778 \nL 135.903272 180.201748 \nL 136.514444 178.67728 \nL 137.125616 186.299621 \nL 137.736788 198.495368 \nL 138.34796 203.068773 \nL 138.959132 195.954588 \nL 139.570304 196.9709 \nL 140.181476 190.36487 \nL 140.792648 188.840402 \nL 141.40382 199.51168 \nL 142.014992 213.740051 \nL 142.626164 212.215583 \nL 143.237336 196.9709 \nL 143.848508 210.691115 \nL 144.45968 212.215583 \nL 145.070852 213.231895 \nL 145.682024 212.723739 \nL 146.293196 199.003524 \nL 146.904367 200.019836 \nL 147.515539 211.199271 \nL 148.126711 190.873026 \nL 148.737883 204.085085 \nL 149.349055 209.674803 \nL 149.960227 208.150334 \nL 150.571399 202.052461 \nL 151.182571 202.052461 \nL 151.793743 212.215583 \nL 152.404915 209.674803 \nL 153.016087 211.707427 \nL 153.627259 188.332246 \nL 154.238431 189.348558 \nL 154.849603 212.215583 \nL 155.460775 192.397495 \nL 156.071947 196.462744 \nL 156.683119 195.954588 \nL 157.294291 210.691115 \nL 157.905463 178.67728 \nL 158.516635 198.495368 \nL 159.127807 201.036149 \nL 159.738979 197.987212 \nL 160.350151 204.085085 \nL 160.961323 208.65849 \nL 161.572495 200.019836 \nL 162.183666 181.726216 \nL 162.794838 191.889339 \nL 163.40601 205.101398 \nL 164.017182 197.987212 \nL 164.628354 204.593241 \nL 165.239526 202.052461 \nL 165.850698 194.938275 \nL 166.46187 183.758841 \nL 167.073042 212.723739 \nL 167.684214 207.642178 \nL 168.295386 200.019836 \nL 168.906558 203.576929 \nL 169.51773 192.397495 \nL 170.128902 207.642178 \nL 170.740074 191.889339 \nL 171.351246 203.576929 \nL 171.962418 200.527993 \nL 172.57359 192.397495 \nL 173.184762 199.51168 \nL 173.795934 211.707427 \nL 174.407106 192.905651 \nL 175.018278 192.397495 \nL 175.62945 192.905651 \nL 176.240622 208.65849 \nL 176.851794 185.283309 \nL 177.462965 176.644655 \nL 178.074137 194.938275 \nL 178.685309 158.351035 \nL 179.296481 203.068773 \nL 179.907653 196.462744 \nL 181.129997 140.565571 \nL 181.741169 150.728694 \nL 182.352341 169.022314 \nL 182.963513 157.334723 \nL 183.574685 176.136499 \nL 184.185857 170.546782 \nL 184.797029 127.861669 \nL 185.408201 168.514158 \nL 186.019373 179.693592 \nL 186.630545 181.21806 \nL 187.241717 201.544305 \nL 187.852889 208.65849 \nL 188.464061 141.073728 \nL 189.075233 185.283309 \nL 189.686405 193.921963 \nL 190.908749 172.579406 \nL 191.519921 204.085085 \nL 192.131093 210.182959 \nL 192.742265 169.53047 \nL 193.353436 212.215583 \nL 193.964608 208.65849 \nL 194.57578 191.889339 \nL 195.186952 191.381183 \nL 195.798124 177.152811 \nL 196.409296 182.742529 \nL 197.020468 152.253162 \nL 197.63164 209.674803 \nL 198.242812 179.185436 \nL 198.853984 170.546782 \nL 199.465156 182.234373 \nL 200.076328 204.085085 \nL 200.6875 171.563094 \nL 201.298672 177.660968 \nL 201.909844 132.94323 \nL 202.521016 175.628343 \nL 203.132188 195.954588 \nL 203.74336 197.479056 \nL 204.354532 179.693592 \nL 204.965704 211.199271 \nL 205.576876 161.399972 \nL 206.188048 175.628343 \nL 206.79922 164.448909 \nL 207.410392 201.036149 \nL 208.021564 212.723739 \nL 208.632735 162.416284 \nL 209.243907 210.182959 \nL 209.855079 141.073728 \nL 210.466251 190.873026 \nL 211.077423 199.003524 \nL 211.688595 112.616985 \nL 212.299767 185.283309 \nL 212.910939 171.054938 \nL 213.522111 187.82409 \nL 214.133283 186.807778 \nL 214.744455 169.53047 \nL 215.355627 180.709904 \nL 215.966799 184.266997 \nL 216.577971 210.691115 \nL 217.189143 188.332246 \nL 217.800315 193.413807 \nL 218.411487 189.856714 \nL 219.022659 205.609554 \nL 219.633831 191.381183 \nL 220.245003 181.21806 \nL 220.856175 204.085085 \nL 221.467347 185.791465 \nL 222.078519 186.299621 \nL 222.689691 201.544305 \nL 223.300863 196.9709 \nL 223.912035 190.873026 \nL 224.523206 180.201748 \nL 225.134378 187.82409 \nL 225.74555 152.253162 \nL 226.356722 184.266997 \nL 226.967894 196.9709 \nL 227.579066 189.348558 \nL 228.190238 199.51168 \nL 228.80141 206.11771 \nL 229.412582 176.644655 \nL 230.023754 180.201748 \nL 230.634926 173.595719 \nL 231.246098 212.723739 \nL 231.85727 157.842879 \nL 232.468442 169.53047 \nL 233.079614 195.446431 \nL 233.690786 180.201748 \nL 234.301958 210.182959 \nL 234.91313 199.003524 \nL 235.524302 209.166646 \nL 236.135474 162.92444 \nL 236.746646 202.560617 \nL 237.357818 152.761318 \nL 237.96899 191.889339 \nL 238.580162 206.625866 \nL 239.191334 196.9709 \nL 239.802505 180.201748 \nL 241.024849 133.451386 \nL 241.636021 132.94323 \nL 242.247193 212.215583 \nL 242.858365 197.987212 \nL 243.469537 187.315934 \nL 244.080709 113.125141 \nL 244.691881 174.103875 \nL 245.303053 180.201748 \nL 245.914225 178.67728 \nL 246.525397 161.399972 \nL 247.136569 204.593241 \nL 247.747741 200.527993 \nL 248.358913 170.546782 \nL 248.970085 176.136499 \nL 249.581257 157.334723 \nL 250.192429 208.65849 \nL 250.803601 183.758841 \nL 251.414773 199.51168 \nL 252.025945 194.938275 \nL 252.637117 192.397495 \nL 253.248289 163.940753 \nL 253.859461 178.169124 \nL 254.470633 183.250685 \nL 255.081804 197.479056 \nL 255.692976 206.625866 \nL 256.304148 204.085085 \nL 256.91532 182.234373 \nL 257.526492 205.609554 \nL 258.137664 182.234373 \nL 258.748836 200.019836 \nL 259.360008 130.402449 \nL 259.97118 179.185436 \nL 260.582352 211.199271 \nL 261.193524 172.579406 \nL 261.804696 185.791465 \nL 262.415868 202.560617 \nL 263.02704 189.348558 \nL 263.638212 203.576929 \nL 264.249384 194.938275 \nL 264.860556 195.446431 \nL 265.471728 191.889339 \nL 266.0829 203.576929 \nL 266.694072 183.250685 \nL 267.305244 205.101398 \nL 267.916416 207.642178 \nL 268.527588 190.36487 \nL 269.13876 205.101398 \nL 269.749932 182.234373 \nL 270.361104 206.625866 \nL 270.972275 193.921963 \nL 271.583447 203.068773 \nL 272.194619 192.397495 \nL 272.805791 205.101398 \nL 273.416963 153.269474 \nL 274.028135 132.94323 \nL 274.639307 194.938275 \nL 275.250479 206.625866 \nL 275.861651 174.103875 \nL 276.472823 191.889339 \nL 277.083995 196.462744 \nL 277.695167 176.644655 \nL 278.306339 195.954588 \nL 278.917511 202.052461 \nL 279.528683 200.019836 \nL 280.139855 211.707427 \nL 280.751027 210.182959 \nL 281.362199 161.908128 \nL 282.584543 197.479056 \nL 283.195715 191.381183 \nL 283.806887 186.807778 \nL 284.418059 184.266997 \nL 285.029231 156.318411 \nL 285.640403 180.709904 \nL 286.251574 184.775153 \nL 286.862746 200.527993 \nL 287.473918 167.497845 \nL 288.08509 156.318411 \nL 288.696262 149.204225 \nL 289.307434 179.693592 \nL 289.918606 176.644655 \nL 290.529778 134.975854 \nL 291.14095 182.234373 \nL 291.752122 168.006001 \nL 292.363294 181.726216 \nL 292.974466 207.642178 \nL 293.585638 196.9709 \nL 294.19681 179.693592 \nL 294.807982 193.921963 \nL 295.419154 190.873026 \nL 296.641498 155.810255 \nL 297.25267 190.873026 \nL 297.863842 188.332246 \nL 298.475014 194.430119 \nL 299.086186 176.644655 \nL 299.697358 200.527993 \nL 300.30853 193.413807 \nL 300.919702 175.120187 \nL 301.530873 196.462744 \nL 302.142045 182.742529 \nL 302.753217 193.921963 \nL 303.364389 192.905651 \nL 303.975561 200.019836 \nL 304.586733 155.810255 \nL 305.197905 202.052461 \nL 305.809077 207.134022 \nL 306.420249 181.726216 \nL 307.031421 172.07125 \nL 307.642593 188.840402 \nL 308.253765 174.612031 \nL 308.864937 199.51168 \nL 309.476109 191.889339 \nL 310.087281 195.446431 \nL 310.698453 209.674803 \nL 311.309625 208.65849 \nL 311.920797 212.215583 \nL 312.531969 196.9709 \nL 313.143141 192.397495 \nL 313.754313 200.019836 \nL 314.365485 201.544305 \nL 315.587829 195.954588 \nL 316.199001 194.430119 \nL 316.810173 202.052461 \nL 317.421344 183.758841 \nL 318.032516 195.446431 \nL 318.643688 197.479056 \nL 319.25486 191.381183 \nL 319.866032 147.679757 \nL 320.477204 209.674803 \nL 321.088376 200.527993 \nL 321.699548 134.467698 \nL 322.31072 206.625866 \nL 322.921892 163.940753 \nL 323.533064 162.416284 \nL 324.755408 201.544305 \nL 325.36658 212.215583 \nL 325.977752 185.791465 \nL 326.588924 207.134022 \nL 327.200096 167.497845 \nL 327.811268 155.302099 \nL 328.42244 175.628343 \nL 329.033612 156.826567 \nL 329.644784 199.003524 \nL 330.255956 176.644655 \nL 330.867128 169.022314 \nL 331.4783 208.150334 \nL 332.089472 180.709904 \nL 332.700643 176.644655 \nL 333.311815 192.905651 \nL 333.922987 157.842879 \nL 334.534159 136.500323 \nL 335.145331 192.905651 \nL 335.756503 191.889339 \nL 336.367675 203.576929 \nL 336.978847 156.826567 \nL 337.590019 211.199271 \nL 338.201191 156.318411 \nL 338.812363 181.21806 \nL 339.423535 195.954588 \nL 340.034707 205.609554 \nL 340.645879 194.938275 \nL 341.257051 180.201748 \nL 341.868223 185.791465 \nL 342.479395 201.544305 \nL 343.090567 189.856714 \nL 343.701739 142.09004 \nL 344.312911 197.479056 \nL 344.924083 188.840402 \nL 345.535255 145.647133 \nL 346.146427 157.842879 \nL 346.757599 209.166646 \nL 347.368771 202.052461 \nL 347.979942 210.182959 \nL 348.591114 213.231895 \nL 349.202286 186.299621 \nL 349.813458 186.299621 \nL 350.42463 194.938275 \nL 351.035802 189.856714 \nL 351.646974 209.166646 \nL 352.258146 207.642178 \nL 352.869318 180.201748 \nL 352.869318 180.201748 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pf0b24398f9)\" d=\"M 48.505682 207.642178 \nL 49.728026 206.625866 \nL 50.339198 205.355476 \nL 50.95037 207.235653 \nL 51.561542 206.795251 \nL 52.172714 207.351803 \nL 53.395058 203.972162 \nL 54.006229 203.475298 \nL 55.228573 200.993802 \nL 55.839745 200.176192 \nL 56.450917 200.273914 \nL 57.062089 200.087591 \nL 57.673261 200.559752 \nL 58.284433 199.720921 \nL 58.895605 198.580061 \nL 59.506777 198.762819 \nL 60.729121 197.164483 \nL 61.340293 196.993998 \nL 61.951465 195.755744 \nL 62.562637 196.081627 \nL 64.396153 194.053707 \nL 65.007325 194.557158 \nL 65.618497 193.483897 \nL 66.229669 193.718701 \nL 66.840841 192.28275 \nL 67.452013 192.222816 \nL 68.063185 192.459089 \nL 68.674357 191.440966 \nL 69.8967 192.609226 \nL 70.507872 192.795779 \nL 71.119044 192.237024 \nL 71.730216 190.74273 \nL 72.341388 191.088993 \nL 72.95256 190.3029 \nL 73.563732 189.118678 \nL 74.174904 188.934943 \nL 74.786076 185.029231 \nL 76.00842 184.620497 \nL 77.230764 182.14968 \nL 78.453108 181.959968 \nL 79.675452 182.674123 \nL 80.286624 182.339839 \nL 80.897796 182.855452 \nL 82.12014 183.686247 \nL 82.731312 183.56271 \nL 83.342484 183.636182 \nL 84.564827 184.00445 \nL 85.787171 184.668604 \nL 87.009515 185.18009 \nL 87.620687 184.923691 \nL 88.231859 185.27561 \nL 88.843031 185.070945 \nL 89.454203 185.462658 \nL 90.065375 185.099195 \nL 90.676547 185.203456 \nL 91.287719 185.498023 \nL 91.898891 184.866903 \nL 92.510063 185.199777 \nL 93.121235 185.30391 \nL 93.732407 185.229106 \nL 94.343579 184.587938 \nL 94.954751 182.768926 \nL 95.565923 182.494965 \nL 96.177095 182.568855 \nL 96.788267 181.440379 \nL 98.010611 181.311016 \nL 99.844127 182.318069 \nL 100.455298 181.56077 \nL 101.06647 181.235583 \nL 101.677642 180.565542 \nL 102.899986 179.834746 \nL 103.511158 179.576325 \nL 104.12233 178.826413 \nL 105.344674 179.385455 \nL 105.955846 179.308463 \nL 106.567018 179.57714 \nL 107.17819 179.37403 \nL 109.622878 180.609279 \nL 112.067566 181.532633 \nL 113.28991 181.237057 \nL 114.512254 181.656287 \nL 116.345769 182.443079 \nL 116.956941 182.301827 \nL 117.568113 182.524111 \nL 118.179285 182.472985 \nL 118.790457 182.295702 \nL 121.235145 183.051657 \nL 121.846317 182.763527 \nL 123.068661 182.031936 \nL 124.291005 181.648977 \nL 124.902177 181.863338 \nL 125.513349 181.514151 \nL 126.124521 181.567418 \nL 126.735693 181.178668 \nL 127.346865 181.354872 \nL 127.958037 181.159874 \nL 128.569209 181.310452 \nL 129.180381 180.675518 \nL 130.402725 179.795223 \nL 131.625068 180.201748 \nL 132.23624 179.840884 \nL 132.847412 180.040893 \nL 133.458584 179.809742 \nL 134.069756 179.808918 \nL 134.680928 179.586235 \nL 138.34796 179.964837 \nL 142.014992 180.650509 \nL 143.848508 181.146854 \nL 146.293196 181.846154 \nL 149.349055 182.488451 \nL 153.016087 183.365906 \nL 154.238431 183.428831 \nL 155.460775 183.643351 \nL 160.961323 184.396096 \nL 162.183666 184.465368 \nL 164.017182 184.68422 \nL 165.850698 184.930496 \nL 166.46187 184.924457 \nL 168.295386 185.257514 \nL 171.351246 185.617887 \nL 175.018278 185.982024 \nL 178.074137 186.115922 \nL 178.685309 185.986179 \nL 179.907653 186.113768 \nL 180.518825 186.039689 \nL 181.741169 185.670807 \nL 184.185857 185.358507 \nL 184.797029 185.101825 \nL 186.630545 184.987818 \nL 187.852889 185.163482 \nL 188.464061 184.971787 \nL 191.519921 185.032475 \nL 192.131093 185.139045 \nL 192.742265 185.073186 \nL 194.57578 185.312952 \nL 196.409296 185.293765 \nL 197.020468 185.158353 \nL 197.63164 185.25842 \nL 201.298672 185.153739 \nL 201.909844 184.946555 \nL 203.132188 184.953208 \nL 204.965704 185.083606 \nL 206.79922 184.876784 \nL 209.243907 185.054254 \nL 209.855079 184.88829 \nL 211.077423 184.963571 \nL 211.688595 184.693621 \nL 214.744455 184.60949 \nL 215.966799 184.594065 \nL 217.189143 184.701773 \nL 225.134378 185.025727 \nL 226.356722 184.910893 \nL 229.412582 185.059173 \nL 233.690786 184.974069 \nL 235.524302 185.180685 \nL 236.135474 185.108424 \nL 236.746646 185.164904 \nL 237.357818 185.060376 \nL 240.413677 185.0865 \nL 241.636021 184.759123 \nL 242.858365 184.886661 \nL 243.469537 184.894252 \nL 244.080709 184.670672 \nL 247.136569 184.595896 \nL 248.358913 184.601636 \nL 250.803601 184.563932 \nL 252.637117 184.662904 \nL 254.470633 184.578205 \nL 258.748836 184.828178 \nL 259.360008 184.670878 \nL 261.804696 184.699656 \nL 266.0829 184.947385 \nL 267.305244 184.998798 \nL 269.13876 185.131705 \nL 270.972275 185.206738 \nL 272.805791 185.328878 \nL 274.639307 185.127164 \nL 275.861651 185.155248 \nL 280.139855 185.362207 \nL 280.751027 185.427353 \nL 281.973371 185.349648 \nL 284.418059 185.397546 \nL 285.640403 185.310742 \nL 288.08509 185.229002 \nL 289.307434 185.123787 \nL 291.14095 184.969223 \nL 292.363294 184.918707 \nL 294.19681 184.992034 \nL 296.030326 184.997941 \nL 297.25267 184.940802 \nL 311.309625 185.238507 \nL 313.754313 185.377931 \nL 318.643688 185.572373 \nL 321.088376 185.587975 \nL 321.699548 185.473868 \nL 322.921892 185.473021 \nL 324.144236 185.415969 \nL 327.200096 185.51904 \nL 329.033612 185.369475 \nL 330.255956 185.380101 \nL 332.089472 185.383848 \nL 339.423535 185.23537 \nL 341.257051 185.287544 \nL 345.535255 185.193573 \nL 346.757599 185.186666 \nL 352.869318 185.44319 \nL 352.869318 185.44319 \n\" style=\"fill:none;stroke:#bfbf00;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 224.64 \nL 33.2875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 224.64 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 7.2 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf0b24398f9\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "print(win_count)\n",
    "env.close()\n",
    "plt.plot(xval, yval, 'b')\n",
    "plt.plot(xval, yavg, 'y')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}